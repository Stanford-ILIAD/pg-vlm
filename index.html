
<!DOCTYPE html>
<html>

  <head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Physically Grounded VLMs</title>

    <meta name="description" content="Physically Grounded VLMs">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->
    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-52J0PM8XKV');
    </script>
    
    <style>
      .nav-pills {
          position: relative;
          display: inline;
      }
      .imtip {
          position: absolute;
          top: 0;
          left: 0;
      }
    </style>
  </head>

  <body>
    <div class="container" id="main">
      <div class="row">
        <h2 class="col-md-12 text-center">
          <strong><font size="+6">Physically Grounded Vision-Language Models</font></strong> </br> for Robotic Manipulation </br> 
          <!--<small>
              Under Review
              </small>-->
        </h2>
      </div>
      <div class="row">
        <div class="col-md-12 text-center">
          <ul class="list-inline">
            <br>
	    <li>Jensen Gao</li> <li>Bidipta Sarkar</li> <li>Fei Xia</li> <li>Ted Xiao</li> <li>Jiajun Wu</li> <li>Brian Ichter</li> <li>Anirudha Majumdar</li> <li>Dorsa Sadigh</li> <br>
	          <br>
              <a href="https://stanford.edu">
                <image src="img/stanford_logo.png" height="55px"> </a>
              <a href="http://g.co/robotics">
                <image src="img/rng-logo.png" height="47px"> </a>
            <br>
          </ul>
        </div>
      </div>

      <div class="row">
        <div class="col-md-4 col-md-offset-4 text-center">
          <ul class="nav nav-pills nav-justified">
            <li>
              <a href="https://arxiv.org/abs/2309.02561">
                <image src="img/arxiv.jpg" height="60px">
                  <h4><strong>Paper</strong></h4>
              </a>
            </li>

	    <li>
              <a href="appendix/appendix.pdf">
                <image src="img/appendix.jpg" height="60px">
                  <h4><strong>Appendix</strong></h4>
              </a>
            </li>

            <li>
              <a href="https://drive.google.com/file/d/17gbzrJSs8YjVafIrX4omR_rx6qLgXjUd/view?usp=sharing">
                <image src="img/youtube_icon.png" height="60px">
                  <h4><strong>Video</strong></h4>
              </a>
            </li>

	    
	    <!-- https://iconduck.com/icons/193266/dataset  -->
            <li>
              <a href="https://drive.google.com/file/d/1ThZ7p_5BnMboK_QE13m1fPKa4WGdRcfC/view?usp=sharing">
                <image src="img/dataset.svg" height="60px">
                  <h4><strong>Dataset</strong></h4>
              </a>
            </li>
            <li>
              <a href="https://huggingface.co/bidiptas/PG-InstructBLIP">
                <image src="img/huggingface.svg" height="60px">
                  <h4><strong>Model</strong></h4>
              </a>                   
            </li> 
          </ul>
        </div>
      </div>


      
      

      <div class="row">
        <div class="col-md-8 col-md-offset-2">
          <p style="text-align:center;">
            <!-- <video id="v0" width="100%" playsinline autoplay muted loop controls> -->
              <!-- <source src="img/rt1_mosaic_comp.mp4" type="video/mp4"> -->
	      <!-- 	</video> -->
	    <div style="position:relative;padding-top:56.25%;">
              <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/UuKAp9a6wMs" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe> -->
	      <iframe src="https://drive.google.com/file/d/17gbzrJSs8YjVafIrX4omR_rx6qLgXjUd/preview" width="560" height="315" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
            </div>
          </p>
          <h3>
            Abstract
          </h3>
          <p class="text-justify">
	    Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PhysObjects, an object-centric dataset of 39.6K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, including generalization to held-out concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically-grounded VLM in an interactive framework with a large language model-based robotic planner, and show improved planning performance on tasks that require reasoning about physical object concepts, compared to baselines that do not leverage physically-grounded VLMs. We additionally illustrate the benefits of our physically-grounded VLM on a real robot, where it improves task success rates.
          </p>

	  <h3>
            PhysObjects Dataset
          </h3>
          <p class="text-justify">
	    To benchmark and improve VLMs for object-centric physical reasoning, we compiled the PhysObjects dataset, which contains 39.6K crowd-sourced and 417K automated physical concept annotations. The source of our images is the <a href="https://ai.meta.com/datasets/egoobjects-dataset/">EgoObjects dataset</a>.

	    We collected annotations for eight physical concepts, listed in the table below. We chose these concepts based on prior work and what we believe to be userful for robotic manipulation. However, we do not consider concepts that would be difficult for humans to estimate from only images, such as friction.

	    <table id="tbl">
	      <tr>
		<th> Concept </th>
		<th> Description </th>
	      </tr>
	      <tr>
		<td> Mass </td>
		<td> How heavy an object is </td>
	      </tr>
	      <tr>
		<td> Fragility </td>
		<td> How easily an object can be broken/damaged </td>
	      </tr>
	      <tr>
		<td> Deformability </td>
		<td> How easily an object can change shape without breaking </td>
	      </tr>
	      <tr>
		<td> Material </td>
		<td> What an object is primarily made of </td>
	      </tr>
	      <tr>
		<td> Transparency </td>
		<td> How much can be seen through an object  </td>
	      </tr>
	      <tr>
		<td> Contents </td>
		<td> What is inside a container </td>
	      </tr>
	      <tr>
		<td> Can Contain Liquid </td>
		<td> If a container can be used to easily carry liquid </td>
	      </tr>
	      <tr>
		<td> Is Sealed </td>
		<td> If a container will not spill if rotated </td>
	      </tr>
	      <tr>
		<td> Density <i>(held-out)</i> </td>
		<td> How much mass per unit of volume of an object </td>
	      </tr>
	      <tr>
		<td> Liquid Capacity <i>(held-out)</i> </td>
		<td> How much liquid a container can contain </td>
	      </tr>
	    </table>
          </p>
          <!-- <p style="text-align:center;"> -->
            <!-- 	    	<video id="v0" width="100%" playsinline autoplay muted loop> -->
	      <!--           <source src="img/RT1-video.mp4" type="video/mp4"> -->
	      <!--       </video> -->
            <!--       RT-1 shows better performance and generalization thanks to its ability to absorb a large amount of diverse data, including robot trajectories with multiple tasks, objects and environments. Baseline approaches exhibit limited ability to fully utilize large datasets. -->
            <!--    </p> -->
        </div>
      </div>


      <!-- <div class="row"> -->
        <!--     <div class="col-md-8 col-md-offset-2"> -->
          <!--         <h3> -->
            <!--             Video -->
            <!--         </h3> -->
          <!--         <div class="text-center"> -->
            <!--             <div style="position:relative;padding-top:56.25%;"> -->
              <!--               <\!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/UuKAp9a6wMs" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe> -\-> -->
		<!-- 	      <iframe src="https://drive.google.com/file/d/14dBpzzK-NsF18Psmaeb3fGhUl9SiyzgU/preview" width="560" height="315" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe> -->
		<!--             </div> -->
            <!--         </div> -->
          <!--     </div> -->
        <!-- </div> -->


      <div class="row">
        <div class="col-md-8 col-md-offset-2">
          <br>
          <h3>
            Real Scene Planning Evaluation
          </h3>
          <p class="text-justify">
            Click on a scene image to go to the planning results for a task in that scene.

	    <div class="irow">
	      <div class="column">
		<h4>
		  Scene 1: Countertop
		</h4>
		<p style="text-align:center;">
		  <a href="scene1.html">
		    <img src="img/eval2_scene1.png" class="img-responsive">
		  </a>
		</p>

	      </div>
	      
	      <div class="column">
		<h4>
		  Scene 2: Art Table
		</h4>
		<p style="text-align:center;">
		  <a href="scene2.html">
		    <img src="img/eval2_scene2.png" class="img-responsive">
		  </a>
		</p>
	      </div>
	    </div>
	    
	    <div class="irow">
	      <div class="column">

		<h4>
		  Scene 3: Floor
		</h4>
		<p style="text-align:center;">
		  <a href="scene3.html">
		    <img src="img/eval2_scene3.png" class="img-responsive">
		  </a>
		</p>

		<h4>
		  Scene 5: Kitchen B
		</h4>
		<p style="text-align:center;">
		  <a href="scene5.html">
		    <img src="img/eval2_scene5.png" class="img-responsive">
		  </a>
		</p>

		<h4>
		  Scene 7: Living Room
		</h4>
		<p style="text-align:center;">
		  <a href="scene7.html">
		    <img src="img/eval2_scene7.png" class="img-responsive">
		  </a>
		</p>
	      </div>
	      
	      <div class="column">

		<h4>
		  Scene 4: Kitchen A
		</h4>
		<p style="text-align:center;">
		  <a href="scene4.html">
		    <img src="img/eval2_scene4.png" class="img-responsive">
		  </a>
		</p>

		<h4>
		  Scene 6: Salad Bar
		</h4>
		<p style="text-align:center;">
		  <a href="scene6.html">
		    <img src="img/eval2_scene6.png" class="img-responsive">
		  </a>
		</p>

		<h4>
		  Scene 8: Shelf
		</h4>
		<p style="text-align:center;">
		  <a href="scene8.html">
		    <img src="img/eval2_scene8.png" class="img-responsive">
		  </a>
		</p>
	      </div>
	    </div>

	</div>



	<div class="row">
          <div class="col-md-8 col-md-offset-2">
            <br>
            <h3>
	      Real Robot Evaluation
            </h3>
            <p class="text-justify">
	      Click on a scene image to go to videos of all tasks for that scene. 
	      <div class="irow">
		<div class="column">
		  <h4>
		    Robot Scene 1
		  </h4>
		  <p style="text-align:center;">
		    <a href="RS1/index.html">
		      <img src="img/RealScene1.png" class="img-responsive">
		    </a>
		  </p>
		</div>
		
		<div class="column">
		  <h4>
		    Robot Scene 2
		  </h4>
		  <p style="text-align:center;">
		    <a href="RS2/index.html">
		      <img src="img/RealScene2.png" class="img-responsive">
		    </a>
		  </p>
		</div>

	      </div>    
	  </div>
	</div>
	
	<!-- <div class="row"> -->
	<!--   <div class="col-md-8 col-md-offset-2"> -->
        <!--     <h3> -->
        <!--       Data -->
        <!--     </h3> -->
	<!--     <p class="text-justify"> -->
	<!--       To test RT-1 in the real world, we collected a large dataset of real-world robotic experiences that consists of over 130k episodes, which contain over 700 tasks, and was collected with a fleet of 13 robots over 17 months. -->
	<!--       <br><br> -->
	<!--       The current set of skills includes picking, placing, opening and closing drawers, getting items in and out drawers, placing elongated items up-right, knocking them over, pulling napkins and opening jars.  -->
	<!--       The list of instructions was designed to show multiple skills with many objects to test aspects of RT-1 such as generalization to new instructions and ability to perform many skills.  -->
	<!--       The entire process of adding tasks and data is described in detail in the paper. -->
	<!--       Since we do not make any assumptions about particular skills when adding new instructions, the system is easily extendable, and we can continuously provide more diverse data to improve its capabilities. -->
	<!--   </div> -->
	<!-- </div> -->


	<!-- <div class="row"> -->
	<!--   <div class="col-md-8 col-md-offset-2"> -->
        <!--     <h3> -->
        <!--       Results -->
        <!--     </h3> -->
	<!--     <p class="text-justify"> -->
	<!--       We test generalization capabilities of our model on multiple axes such as previously unseen instructions, robustness to the number of distractor objects (first row in the image below), robustness to different backgrounds and environments such as new, previously unseen kitchens (second row), and realistic scenarios that combine all these elements. -->
	      
	<!--     <p style="text-align:center;"> -->
        <!--       <image src="img/evals.png" class="img-responsive">        	    -->
        <!--     </p> -->
	<!--     We first compare RT-1 to other previously published imitation-learning-based baselines such as Gato and BC-Z (including a BC-Z with a similar number of parameters as RT-1 that we call BC-Z XL). -->
	<!--     </p> -->
	<!--     <p class="text-justify"> -->
	<!--       Across each category, we find that RT-1 outperforms the prior models significantly. -->
	<!--       On seen tasks, RT-1 is able to perform 97% of the more than 700 instructions successfully, which is 25% more than BC-Z and 32% more than Gato. -->
	<!--       On unseen tasks, RT-1 shows it is capable of generalizing to novel instructions, performing 76% of the never-before-seen instructions, 24% more than the next best baseline.  -->
	<!--       On distractors and backgrounds, we find that RT-1 is quite robust, successfully executing 83% of the distractor robustness tasks and 59% of the background robustness tasks (36% and 18% higher than the next best alternative, respectively). -->
	<!--     </p> -->
        <!--     <p style="text-align:center;"> -->
        <!--       <image src="img/main_baselines.png"  class="img-responsive" height="600px"> -->
        <!--     </p> -->
	    
	<!--     <p class="text-justify"> -->
	<!--       Next, we test whether our method generalizes enough across all the different axes that we evaluated previously to be deployed in a real kitchen, which poses multiple distribution shifts all at once such as new tasks combinations, object distractors as well as a novel environment. -->
	<!--       The office kitchen involves a dramatic shift from the training environment and we categorize tasks across these scenarios with varying levels of generalization: L1 for generalization to the new counter-top layout and lighting conditions, L2 for additionally generalization to unseen distractor objects, L3 for additional generalization to drastically new task settings, new task objects or objects in unseen locations such as near a sink. The three levels that correspond to three tasks of restocking, preparing a snack and fetching a lost object in the real kitchen. -->
	<!--       <br><br> -->
	<!--       Simiarly to the previous experiment, RT-1 generalizes better than the baselines. Gato generalizes fairly well at the first level but it performs significantly drops for the more difficult generalization scenarios. BC-Z and its XL equivalent perform fairly well at L2 level and better than Gato at L3 but they are still not at the generalization level of RT-1.  -->
	<!--     </p> -->
	    
	<!--     <p style="text-align:center;"> -->
        <!--       <image src="img/kanishka.png"  class="img-responsive" height="450px"> -->
        <!--     </p> -->
	    
	<!--     <p> Given these initial results, we try to push RT-1 further by incorporating data from different data sources such as simulation (green box below) or data collected by another robot (red box below).  -->
	<!--     <p style="text-align:center;"> -->
        <!--       <image src="img/multi_simple.png"  class="img-responsive" height="600px"> -->
        <!--     </p> -->
	    
        <!--     <p class="text-justify"> -->
	<!--       Our results indicate that RT-1’s absorption properties also include the ability to acquire new skills by observing other simulation or robots’ experiences without sacrificing the performance of the original tasks. In the left plot below, we see that by mixing real and sim data, the generalization capabilities of the robot improve significantly when evaluated on objects seen only in simulation (and they only drop by 2% on all other objects). -->
	<!--       <br> -->
	<!--       Even more interestingly, we observe that mixing our original dataset with data from another robot (in this the Kuka IIWA robot) improves generalization as well: the 22% accuracy seen when training with our data alone jumps to 39% when RT-1 is trained on both bin-picking data from Kuka and the existing data. That’s almost a 2x improvement (17%) that shows an effective transfer from a different robot morphology and presents an exciting avenue for future work where we combine many more multi-robot datasets to enhance the robot capabilities. -->
	<!--     </p> -->
	    
        <!--     <p style="text-align:center;"> -->
        <!--       <image src="img/multi_results.png"  class="img-responsive" height="600px"> -->
        <!--     </p> -->
            
        <!--     Given these results, we put everything together to evaluate the ability of RT-1 to execute long-horizon instructions in the <a href="https://say-can.github.io/">(PaLM-)SayCan framework</a>. We implement two other baselines for comparison: (1) SayCan with Gato, and (2) SayCan with BC-Z. We evaluate all three policies in two real kitchens. Kitchen2 constitutes a much more challenging generalization scene than Kitchen1; the mock kitchen used to gather most of the training data was modeled after Kitchen1. -->
        <!--     <p style="text-align:center;"> -->
        <!--       <image src="img/saycan_table.png"  class="img-responsive" height="600px"> -->
        <!--     </p> -->
            

	<!--     <p class="text-justify"> -->
	<!--       We see that RT-1 achieves a 67% execution success rate in Kitchen1, and is better than other baselines. Due to the generalization difficulty presented by the new unseen kitchen, the performance of SayCan with Gato and SayCan with BCZ shapely falls, while RT-1 does not show a visible drop. -->
	<!--       <br><br> -->
	      
	<!--       Below, we show a few example videos showing how PaLM-SayCan-RT1 can be used to plan and execute ultra-long horizon tasks, with as many as 50 steps.  -->
	<!--       The first task "Bring me the rice chips from the drawer" is executed in an office kitchen that the robot has never seen before. -->
	<!--     </p> -->
	<!--     <p style="test-align:center;"> -->
	<!--       <video id="v0" width="100%" playsinline muted loop controls> -->
	<!-- 	<source src="img/saycan_rt1_demo1_comp.mp4" type="video/mp4"> -->
        <!--       </video>		 -->
        <!--     </p> -->
	<!--     <p class="text-justify"> -->

        <!--       For the second task "Roses are red, violets are blue, bring me the rice chips from the drawer, and a napkin too." the execution  -->
	<!--       and planning process are shown in the video below. -->
	<!--     </p> -->
        <!--     <p style="test-align:center;"> -->
	<!--       <video id="v0" width="100%" playsinline muted loop controls> -->
	<!-- 	<source src="img/saycan_rt1_demo2_comp.mp4" type="video/mp4"> -->
        <!--       </video>		 -->
        <!--     </p> -->
        <!--     <p class="text-justify"> -->

        <!--       In the next example, we show SayCan is able to plan and execute a very long-horizon task involving 50+ steps. -->
	<!--     </p> -->
        <!--     <p style="test-align:center;"> -->
	<!--       <video id="v0" width="100%" playsinline muted loop controls> -->
	<!-- 	<source src="img/saycan_rt1_demo3_comp.mp4" type="video/mp4"> -->
        <!--       </video>		 -->

	<!--   </div> -->
	<!-- </div> -->
	
	

	<div class="row">
	  <div class="col-md-8 col-md-offset-2">
            <h3>
              Citation
            </h3>
            <div class="form-group col-md-10 col-md-offset-1">
              <textarea id="bibtex" class="form-control" readonly>@inproceedings{pgvlm2023,
	title={Physically Grounded Vision-Language Models for Robotic Manipulation},
	author={Jensen Gao and Bidipta Sarkar and Fei Xia and Ted Xiao and Jiajun Wu and Brian Ichter and Anirudha Majumdar and Dorsa Sadigh},
	booktitle={arXiv preprint arXiv:2309.02561},
	year={2023}
}</textarea>
            </div>
	  </div>
	  
	</div>


	<div class="row">
	  <div id="open-source" class="col-md-8 col-md-offset-2">
            <h3>
              Open Source
            </h3>
            We open source the PG-InstructBLIP model <a href="https://huggingface.co/bidiptas/PG-InstructBLIP">[here]</a>.
            We also open source the dataset <a href="https://drive.google.com/file/d/1ThZ7p_5BnMboK_QE13m1fPKa4WGdRcfC/view?usp=sharing">[here]</a>.
            <p style="text-align:center;">
            </p>
	  </div>
	</div>

	<div class="row">
	  <div class="col-md-8 col-md-offset-2">
            <!-- <h3> -->
              <!--     Acknowledgements -->
              <!-- </h3> -->
            <p class="text-justify">
              <br><br>
              The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a> and <a href="https://robotics-transformer.github.io/">RT-1</a>
            </p>
	  </div>
	</div>
      </div>
  </body>
</html>
